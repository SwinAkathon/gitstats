{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Git repos analysis\n",
    "==================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up\n",
    "Ensure that necessary resources are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this ONCE\n",
    "# %pip install PyGithub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "FOLDER_OUTPUT = 'output'\n",
    "FOLDER_CHART = 'output/charts'\n",
    "FILE_GIT_TOKEN = 'github-token'\n",
    "\n",
    "FILE_TEAMS = 'data/teams-cos40005-fall2025.csv'\n",
    "  # 'data/teams.csv'\n",
    "\n",
    "def ensure_configuration():\n",
    "  # ensure folders exist and created\n",
    "  import os\n",
    "  import sys\n",
    "\n",
    "  REQUIRED_FOLDERS = [FOLDER_OUTPUT, FOLDER_CHART]\n",
    "  REQUIRED_FILES = [FILE_GIT_TOKEN, FILE_TEAMS]\n",
    "\n",
    "  print(\"Checking required configuration...\")\n",
    "\n",
    "  # 1. Ensure folders exist, create them if necessary\n",
    "  try:\n",
    "      for folder in REQUIRED_FOLDERS:\n",
    "          os.makedirs(folder, exist_ok=True)\n",
    "          print(f\"✔️ Directory '{folder}' is ready.\")\n",
    "  except OSError as e:\n",
    "      print(f\"❌ Error creating directory {folder}: {e}\")\n",
    "      sys.exit(1)\n",
    "\n",
    "  # 2. Ensure all required files exist, otherwise stop the program\n",
    "  try:\n",
    "      for file in REQUIRED_FILES:\n",
    "          if not os.path.exists(file):\n",
    "              # Raise an error with a specific message for the missing file\n",
    "              raise FileNotFoundError(f\"❌ Required file '{file}' was not found.\")\n",
    "          else:\n",
    "              print(f\"✔️ Required file '{file}' found.\")\n",
    "  except FileNotFoundError as e:\n",
    "      print(f\"❌ Critical Error: {e}\")\n",
    "      print(\"Please create the missing file before running the program.\")\n",
    "      sys.exit(1)\n",
    "\n",
    "\n",
    "  print(\"\\nConfiguration check...DONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure configuration (ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute this ONCE\n",
    "# ensure_configuration()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Read Git contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from github import Github\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to read the access token from a file\n",
    "# @requires: access_token is kept in the single-line data file specified by `file_path`\n",
    "def read_access_token(file_path='token'):\n",
    "    with open(file_path, 'r') as file:\n",
    "        token = file.readline().strip()\n",
    "    return token\n",
    "\n",
    "# personal access token stored in a file in the current directory\n",
    "token_path = os.path.join(os.getcwd(), FILE_GIT_TOKEN)\n",
    "token = read_access_token(token_path)\n",
    "\n",
    "# Replace 'your_github_token' with your actual GitHub token\n",
    "# Initialize the GitHub object\n",
    "g = Github(token)\n",
    "\n",
    "def read_repo(repo_name):\n",
    "    # Get the repository\n",
    "    repo = g.get_repo(repo_name)\n",
    "\n",
    "    # List to store commit data\n",
    "    data = []\n",
    "\n",
    "    # Get commits and iterate through them\n",
    "    commits = repo.get_commits()\n",
    "    # sort commits by date\n",
    "    commits = sorted(commits, key=lambda x: x.commit.author.date)\n",
    "    for commit in commits:\n",
    "        if 'merge' in commit.commit.message.lower():\n",
    "            continue\n",
    "        commit_data = commit.stats\n",
    "        commit_info = {\n",
    "            'date-time': commit.commit.author.date.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'who': commit.commit.author.name,\n",
    "            'comments': commit.commit.message.replace('\\n', ' ').strip(),\n",
    "            'lines_added': commit_data.additions,\n",
    "            'lines_deleted': commit_data.deletions\n",
    "        }\n",
    "        data.append(commit_info)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def save_data(data, csv_file):\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "\n",
    "def process_teams(file_team):\n",
    "    with open(file_team, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            team_name = row[0]\n",
    "            print(f\"Processing team {team_name}\", end='...\\n')\n",
    "            # some teams have multiple repositories\n",
    "            repos = row[1].split(';')\n",
    "            data = []\n",
    "            \n",
    "            # read each repo\n",
    "            for repo in repos:\n",
    "                print('repo URL: ', repo, end='...')\n",
    "                repo_name1 = repo.split('/')[-2]\n",
    "                repo_name2 = repo.split('/')[-1]\n",
    "                repo_name = f'{repo_name1}/{repo_name2}'\n",
    "                print('repo name: ', repo_name)\n",
    "                data += read_repo(repo_name)\n",
    "            \n",
    "            # sort data by date-time, add column 'total_lines' = 'lines_added' - 'lines_deleted'\n",
    "            data = sorted(data, key=lambda x: datetime.strptime(x['date-time'], '%Y-%m-%d %H:%M:%S'))\n",
    "            \n",
    "            total_lines = 0\n",
    "            for row in data:\n",
    "                line_changes = row['lines_added'] - row['lines_deleted']\n",
    "                total_lines += line_changes\n",
    "                row['total_lines'] = total_lines\n",
    "\n",
    "            save_data(data, f'{FOLDER_OUTPUT}/{team_name}.csv')\n",
    "            print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (exec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "process_teams(FILE_TEAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate Git contribution graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "def read_teams(teams_file):\n",
    "    team_names = []\n",
    "    with open(teams_file, 'r') as file:\n",
    "        reader = csv.reader(file)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            team_names.append(row[0])\n",
    "    \n",
    "    return team_names\n",
    "\n",
    "def process_team(name, team_file):\n",
    "\n",
    "    df = pd.read_csv(team_file)\n",
    "    team_name = team_file.split('.')[0]\n",
    "\n",
    "    # Display the first few rows of the dataframe to understand its structure\n",
    "    df.head()\n",
    "\n",
    "    # Convert the date-time column to datetime format\n",
    "    df['date-time'] = pd.to_datetime(df['date-time'])\n",
    "\n",
    "    # Draw pie chart for 'who' commit\n",
    "    commit_counts = df['who'].value_counts()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)  # Add this line to create a subplot for the pie chart\n",
    "    commit_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Commit Distribution for ' + team_name)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # Draw line chart for total lines with x-axis as date-time\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(df['date-time'], df['total_lines'], marker='o')\n",
    "    plt.title('Total Lines Over Time')\n",
    "    plt.xlabel('Date-Time')\n",
    "    plt.ylabel('Total Lines')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot to a file\n",
    "    plt.savefig(f'{FOLDER_CHART}/{name}.png')\n",
    "\n",
    "    print(f'Saved analytic chart to: {FOLDER_CHART}/{name}.png')\n",
    "    \n",
    "    # plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main (execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_names = read_teams(FILE_TEAMS)\n",
    "for name in team_names:\n",
    "    process_team(name, f'output/{name}.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
